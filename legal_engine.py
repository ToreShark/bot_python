
import bs4
from dotenv import load_dotenv
from langchain import hub
from operator import itemgetter
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
import os
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain.load import dumps, loads
from langchain_anthropic import ChatAnthropic


from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from utils import format_qa_pair, format_qa_pairs

from colorama import Fore
import warnings
import os
import hashlib

warnings.filterwarnings("ignore")

load_dotenv()

# LLM
# llm = ChatOpenAI(model="gpt-4-turbo")
llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å, –Ω–∞–ø—Ä–∏–º–µ—Ä claude-3-7-sonnet-latest
    temperature=0.2,
    max_tokens=4000,
    # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º anthropic_api_key, –ø–æ—Å–∫–æ–ª—å–∫—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å–∞–º–∞ –≤–æ–∑—å–º–µ—Ç –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
)

# # 1. Load legal documents
# docs = []
# for filename in os.listdir("docs"):
#     if filename.endswith(".txt"):
#         loader = TextLoader(os.path.join("docs", filename), encoding='utf-8')
#         docs.extend(loader.load())

docs_path = "./docs"  # –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
docs = []

for filename in os.listdir(docs_path):
    if filename.endswith(".txt"):
        with open(os.path.join(docs_path, filename), "r", encoding="utf-8") as f:
            content = f.read()
            docs.append(Document(page_content=content, metadata={"source": filename}))
            # print(Fore.YELLOW + f"[LOG] –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {filename}" + Fore.RESET)



# 2. Split into chunks
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500,
    chunk_overlap=50
)
splits = text_splitter.split_documents(docs)
# print(Fore.YELLOW + f"[LOG] –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è: {len(splits)}" + Fore.RESET)
for s in splits[:3]:
    print(Fore.LIGHTBLACK_EX + s.page_content[:200] + "..." + Fore.RESET)


# 3. Create vectorstore
# # –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
persist_directory = "./chroma_db"
hash_file = "./docs_hash.txt"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
def get_documents_hash(documents):
    """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à-—Å—É–º–º—É –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    content = "".join([doc.page_content for doc in documents])
    return hashlib.md5(content.encode()).hexdigest()

# –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ö–µ—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
current_hash = get_documents_hash(splits)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å embeddings
need_update = True
if os.path.exists(hash_file):
    with open(hash_file, "r") as f:
        stored_hash = f.read().strip()
    
    # –ï—Å–ª–∏ —Ö–µ—à –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    if current_hash == stored_hash and os.path.exists(persist_directory):
        need_update = False
        print(Fore.GREEN + f"[LOG] –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ embeddings" + Fore.RESET)

# –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
if need_update:
    print(Fore.YELLOW + f"[LOG] –î–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –∏–ª–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ embeddings..." + Fore.RESET)
    
    # –°–æ–∑–¥–∞–µ–º embeddings –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Chroma
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=OpenAIEmbeddings(model="text-embedding-3-large"),
        persist_directory=persist_directory
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ —Ö–µ—à
    vectorstore.persist()
    with open(hash_file, "w") as f:
        f.write(current_hash)
    
    print(Fore.GREEN + f"[LOG] –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ {persist_directory}" + Fore.RESET)
else:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    vectorstore = Chroma(
        persist_directory=persist_directory, 
        embedding_function=embedding_function
    )
    print(Fore.GREEN + f"[LOG] –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ {persist_directory}" + Fore.RESET)

# –°–æ–∑–¥–∞–µ–º retriever –∫–∞–∫ –æ–±—ã—á–Ω–æ
retriever = vectorstore.as_retriever()

# 1. DECOMPOSITION
# template = """–í—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤—É –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ. 
#     –ö–ª–∏–µ–Ω—Ç –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: {question}
    
#     –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ 3 –ø–æ–∏—Å–∫–æ–≤—ã—Ö –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ:
#     1. –ù–∞–π–¥—É—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–∞
#     2. –ù–∞–π–¥—É—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞
#     3. –ù–∞–π–¥—É—Ç —É—Å–ª–æ–≤–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä
    
#     –ü–æ–¥–≤–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –∞ –Ω–µ –Ω–∞ –¥–∏–∞–ª–æ–≥ —Å –∫–ª–∏–µ–Ω—Ç–æ–º.
#     –í—ã–≤–µ–¥–∏—Ç–µ —Ç–æ–ª—å–∫–æ —Å–∞–º–∏ –≤–æ–ø—Ä–æ—Å—ã (3 —à—Ç—É–∫–∏)."""
template = """–í—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤—É —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ.
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: {question}

–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ 3 —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–æ–≥–æ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –ó–∞–∫–æ–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω "–û –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ –≥—Ä–∞–∂–¥–∞–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω" (—Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è–º–∏ –æ—Ç 20.08.2024 –≥.).

–¶–µ–ª—å:
1. –ù–∞–π—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏–º—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É (–≤–Ω–µ—Å—É–¥–µ–±–Ω–æ–µ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ, —Å—É–¥–µ–±–Ω–æ–µ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏).
2. –£—Ç–æ—á–Ω–∏—Ç—å —É—Å–ª–æ–≤–∏—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä.
3. –ü–æ–ª—É—á–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–π—Å—Ç–≤–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ–ª–æ–∂–µ–Ω–∏—è—Ö —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞.

‚öñÔ∏è –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ:
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —è–∑—ã–∫.
- –°—Å—ã–ª–∞–π—Ç–µ—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ó–∞–∫–æ–Ω–∞ –†–ö "–û –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ –≥—Ä–∞–∂–¥–∞–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω" (–≤ —Ä–µ–¥. –æ—Ç 20.08.2024 –≥.), –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ.
- –ù–µ —É–ø–æ–º–∏–Ω–∞–π—Ç–µ –¥—Ä—É–≥–∏–µ –∑–∞–∫–æ–Ω—ã –∏–ª–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã.
- –ù–µ –≤–µ–¥–∏—Ç–µ –¥–∏–∞–ª–æ–≥ ‚Äî —Ü–µ–ª—å: –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –∞ –Ω–µ –≤–µ—Å—Ç–∏ –±–µ—Å–µ–¥—É.
"""
# template = """–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 5 –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π —Å–ª–µ–¥—É—é—â–µ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:

# {question}

# –§–æ—Ä–º—É–ª–∏—Ä—É–π –∫–∞–∫ —é—Ä–∏—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –æ–¥–Ω–æ–≥–æ —Å–º—ã—Å–ª–∞, –∫–∞–∂–¥—É—é —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏."""

prompt_decomposition = ChatPromptTemplate.from_template(template)

def generate_multi_queries_for_subquestion(question):
    """–°–æ–∑–¥–∞—ë—Ç 5 —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –æ–¥–Ω–æ–≥–æ –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞"""
    chain = prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split("\n"))
    return chain.invoke({"question": question})

def get_unique_union(documents: list[list]):
    """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–ø–∏—Å–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã"""
    flattened = [dumps(doc) for sublist in documents for doc in sublist]
    unique_docs = list(set(flattened))
    return [loads(doc) for doc in unique_docs]

def retrieve_documents(sub_questions):
    """–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞ ‚Üí 5 —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ ‚Üí –ø–æ–∏—Å–∫ ‚Üí –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ"""
    all_retrieved_docs = {}

    for sub_q in sub_questions:
        # print(Fore.BLUE + f"[LOG] –ó–∞–ø—Ä–æ—Å: {sub_q}" + Fore.RESET)
        multi_qs = generate_multi_queries_for_subquestion(sub_q)

        doc_lists = []
        for q in multi_qs:
            docs_found = retriever.get_relevant_documents(q)
            # print(Fore.LIGHTMAGENTA_EX + f"[RETRIEVER] –ó–∞–ø—Ä–æ—Å: {q} ‚Äî –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs_found)}" + Fore.RESET)
            for d in docs_found[:1]:
                # print(Fore.LIGHTCYAN_EX + f"–§—Ä–∞–≥–º–µ–Ω—Ç: {d.page_content[:100]}..." + Fore.RESET)
                 pass  # –∏–ª–∏ –æ—Å—Ç–∞–≤—å print, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω–æ
            doc_lists.append(docs_found)
        
        unique_docs = get_unique_union(doc_lists)
        all_retrieved_docs[sub_q] = unique_docs  # üî• –í—Å—Ç–∞–≤—å —ç—Ç–æ!
        # print(Fore.YELLOW + f"[LOG] –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞: {len(unique_docs)}" + Fore.RESET)

    return all_retrieved_docs


def generate_sub_questions(query):
    """ generate sub questions based on user query"""
    pass 
    # Chain
    generate_queries_decomposition = (
        prompt_decomposition 
        | llm 
        | StrOutputParser()
        | (lambda x: x.split("\n"))
    ) 

    # Run
    sub_questions = generate_queries_decomposition.invoke({"question": query})
    questions_str = "\n".join(sub_questions)
    # print(Fore.MAGENTA + "=====  –ü–û–ò–°–ö–û–í–´–ï –ü–û–î–í–û–ü–†–û–°–´: =====" + Fore.RESET)
    # print(Fore.WHITE + questions_str + Fore.RESET + "\n")
    return sub_questions 
      

# 2. ANSWER SUBQUESTIONS RECURSIVELY 
template = """–í–æ—Ç –≤–æ–ø—Ä–æ—Å, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å:

\n --- \n {sub_question} \n --- \n

–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –ø–∞—Ä—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –∏–º–µ—é—Ç—Å—è:

\n --- \n {q_a_pairs} \n --- \n

–ö–æ–Ω—Ç–µ–∫—Å—Ç, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:

\n --- \n {context} \n --- \n

–ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –¥–∞–π—Ç–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, —Ç–æ—á–Ω—ã–π –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: \n {sub_question}

–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —É–∫–∞–∑–∞–Ω–∞ —Å—Ç–∞—Ç—å—è –∑–∞–∫–æ–Ω–∞, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–µ—ë.
"""

prompt_qa = ChatPromptTemplate.from_template(template)


def generate_qa_pairs(retrieved_docs_dict):
    q_a_pairs = ""

    for sub_question, context_docs in retrieved_docs_dict.items():
        context = "\n".join([doc.page_content for doc in context_docs])

        inputs = {
            "sub_question": sub_question,
            "q_a_pairs": q_a_pairs,
            "context": context
        }

        generate_qa = prompt_qa | llm | StrOutputParser()
        # print(Fore.LIGHTYELLOW_EX + "[CONTEXT] " + context[:500] + "..." + Fore.RESET)
        answer = generate_qa.invoke(inputs)
       

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        q_a_pair = format_qa_pair(sub_question, answer)
        q_a_pairs += "\n --- \n" + q_a_pair

        # print(Fore.GREEN + f"=====  Q/A PAIR: =====" + Fore.RESET)
        # print(Fore.CYAN + f"Q: {sub_question}\nA: {answer}" + Fore.RESET + "\n")

    return q_a_pairs
        

# 3. ANSWER INDIVIDUALY

# RAG prompt = https://smith.langchain.com/hub/rlm/rag-prompt
prompt_rag = hub.pull("rlm/rag-prompt")


def retrieve_and_rag(prompt_rag, sub_questions):
    """RAG on each sub-question"""
    rag_results = []
    for sub_question in sub_questions:
        retrieved_docs = retriever.get_relevant_documents(sub_question)

        answer_chain = (
            prompt_rag
            | llm
            | StrOutputParser()
        )
        answer = answer_chain.invoke({"question": sub_question, "context": retrieved_docs})
        rag_results.append(answer)
    
    return rag_results, sub_questions
    
# SUMMARIZE AND ANSWER 

# Prompt
template = """–í–æ—Ç –Ω–∞–±–æ—Ä –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:

{context}

–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å—Ñ–æ—Ä–º–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–ü–æ—Å—Ç–∞—Ä–∞–π—Å—è —Å–¥–µ–ª–∞—Ç—å –æ—Ç–≤–µ—Ç –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–º –∏ —Ç–æ—á–Ω—ã–º. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏ "–Ω–µ –∑–Ω–∞—é".
–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤—É, –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏ "–Ω–µ –∑–Ω–∞—é".
–î–∞–≤–∞–π –æ—Ç–≤–µ—Ç –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.
–ü—Ä–∏–º–µ—Ä –Ω–∞—á–∞–ª–∞:  
"–°–æ–≥–ª–∞—Å–Ω–æ —Å—Ç. 6 –ó–∞–∫–æ–Ω–∞ –†–ö "–û –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ –≥—Ä–∞–∂–¥–∞–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω", –µ—Å–ª–∏ —É –¥–æ–ª–∂–Ω–∏–∫–∞ –Ω–µ—Ç 12 –º–µ—Å—è—Ü–µ–≤ –ø—Ä–æ—Å—Ä–æ—á–µ–∫, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–ª–∞—Ç–µ–∂–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏..."  

‚öñÔ∏è –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ:
- –ü—Ä–∏–≤–æ–¥–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–∞, –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É–∫–∞–∑–∞–Ω–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ù–∞–ø—Ä–∏–º–µ—Ä: "—Å—Ç. 6 –ó–∞–∫–æ–Ω–∞ –†–ö –æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ".
- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—Ç–µ–∫–∞–µ–º—ã–µ —Ñ—Ä–∞–∑—ã ("–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é", "–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Ç—É–∞—Ü–∏–∏").
- –û—Ç–≤–µ—á–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ: "–î–∞, —ç—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–∫–∞–∑–∞" –∏–ª–∏ "–ù–µ—Ç, –∑–∞–∫–æ–Ω —ç—Ç–æ–≥–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç".
"""

prompt = ChatPromptTemplate.from_template(template)


def query(query_text, progress_callback=lambda x: None):
    # –®–∞–≥ 1: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥–≤–æ–ø—Ä–æ—Å–æ–≤
    progress_callback("üîç –ì–µ–Ω–µ—Ä–∏—Ä—É—é —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã...")
    sub_questions = generate_sub_questions(query_text)

    # –®–∞–≥ 2: –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    progress_callback("üìö –ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
    retrieved_docs_dict = retrieve_documents(sub_questions)

    # –®–∞–≥ 3: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è Q/A –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    progress_callback("‚öñÔ∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ —Å—É–¥–µ–±–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É...")
    q_a_pairs = generate_qa_pairs(retrieved_docs_dict)

    # –®–∞–≥ 4: –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    progress_callback("üß† –§–æ—Ä–º–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥...")
    final_rag_chain = (
        prompt
        | llm
        | StrOutputParser()
    )

    return final_rag_chain.invoke({"question": query_text, "context": q_a_pairs})